# Guide d'Installation - Vocalyx Enrichment

## üöÄ Installation pour Production CPU-Only

### Pr√©requis

- Python 3.8+
- 4+ GB RAM (8+ GB recommand√©)
- CPU avec support AVX2 (la plupart des CPUs modernes)
- Espace disque : 2-7 GB selon le mod√®le choisi

### 1. Installation des D√©pendances

#### Installation Standard

```bash
pip install -r requirements.txt
```

#### Installation Optimis√©e pour CPU (Recommand√©)

Pour de meilleures performances CPU, installez `llama-cpp-python` avec optimisations :

**Linux (avec OpenBLAS) :**
```bash
CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

**macOS (Apple Silicon M1/M2) :**
```bash
CMAKE_ARGS="-DLLAMA_METAL=ON" pip install llama-cpp-python
```

**macOS (Intel) ou Linux sans OpenBLAS :**
```bash
pip install llama-cpp-python
```

**Windows :**
```bash
pip install llama-cpp-python
```

### 2. T√©l√©chargement des Mod√®les

Les mod√®les GGUF quantis√©s seront t√©l√©charg√©s automatiquement au premier usage, ou vous pouvez les t√©l√©charger manuellement :

#### Option A : T√©l√©chargement Automatique (Recommand√©)

Le mod√®le sera t√©l√©charg√© automatiquement depuis Hugging Face Hub lors du premier chargement.

#### Option B : T√©l√©chargement Manuel

```bash
# Installer huggingface_hub si pas d√©j√† fait
pip install huggingface-hub

# T√©l√©charger un mod√®le recommand√©
python -c "
from infrastructure.models.model_manager import ModelManager
manager = ModelManager('./models/enrichment')
manager.download_model('phi-3-mini')
"
```

#### Mod√®les Recommand√©s

| Mod√®le | Taille | RAM Requise | Vitesse | Qualit√© |
|--------|--------|-------------|---------|---------|
| **phi-3-mini** | 2.3 GB | 4 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **mistral-7b-instruct** | 4.1 GB | 6 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **llama-3-8b-instruct** | 4.6 GB | 6 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **phi-3-medium** | 7.0 GB | 8 GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **gemma-2b** | 1.4 GB | 3 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

**Recommandation pour production CPU :** `phi-3-mini` (bon √©quilibre vitesse/qualit√©)

### 3. Configuration

Copier `config.ini.example` vers `config.ini` :

```bash
cp config.ini.example config.ini
```

√âditer `config.ini` et configurer :

```ini
[LLM]
# Mod√®le √† utiliser (nom ou chemin)
model = phi-3-mini

# Param√®tres CPU
n_threads = 0  # 0 = auto-d√©tection (recommand√©)
n_ctx = 2048   # Taille du contexte
n_batch = 512  # Batch size

[PERFORMANCE]
max_workers = 2
max_chunk_size = 500
enable_cache = true
cache_max_models = 2
```

### 4. Test de l'Installation

```bash
# Tester le chargement du mod√®le
python -c "
from enrichment_service import EnrichmentService
from config import Config

config = Config()
service = EnrichmentService(config, 'phi-3-mini')
result = service.enrich_text('Bonjour, comment allez-vous ?')
print(f'R√©sultat: {result}')
"
```

### 5. D√©marrage du Worker

```bash
celery -A worker.celery_app worker \
  --loglevel=info \
  --concurrency=2 \
  --hostname=enrichment-worker-01@%h \
  --without-gossip \
  --without-mingle \
  -Q enrichment
```

## üê≥ Installation Docker

### Dockerfile Optimis√© CPU

Cr√©er un `Dockerfile` :

```dockerfile
FROM python:3.11-slim

# Installer les d√©pendances syst√®me pour llama-cpp-python
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copier les requirements
COPY requirements.txt .

# Installer llama-cpp-python avec optimisations CPU
RUN CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" \
    pip install --no-cache-dir llama-cpp-python

# Installer les autres d√©pendances
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code
COPY . .

# Cr√©er le r√©pertoire des mod√®les
RUN mkdir -p /app/models/enrichment

# Exposer les volumes
VOLUME ["/app/models/enrichment", "/app/logs"]

CMD ["celery", "-A", "worker.celery_app", "worker", \
     "--loglevel=info", "--concurrency=2", \
     "--hostname=enrichment-worker-01@%h", \
     "--without-gossip", "--without-mingle", "-Q", "enrichment"]
```

### Build et Run

```bash
docker build -t vocalyx-enrichment .
docker run -d \
  -v ./models/enrichment:/app/models/enrichment \
  -v ./logs:/app/logs \
  -e LLM_MODEL=phi-3-mini \
  -e MAX_WORKERS=2 \
  vocalyx-enrichment
```

## üîß Optimisations CPU

### Param√®tres Recommand√©s

Pour un CPU avec N cores :

```ini
[LLM]
n_threads = N-1  # Laisser 1 core libre
n_ctx = 2048     # Contexte suffisant pour la plupart des cas
n_batch = 512    # Bon √©quilibre m√©moire/vitesse
```

### Monitoring des Performances

```python
import psutil
import time

# Avant enrichissement
mem_before = psutil.virtual_memory()
start = time.time()

# Enrichissement
result = service.enrich_text(text)

# Apr√®s enrichissement
mem_after = psutil.virtual_memory()
elapsed = time.time() - start

print(f"Temps: {elapsed:.2f}s")
print(f"M√©moire: {mem_after.used - mem_before.used} MB")
```

## üêõ D√©pannage

### Erreur: "llama-cpp-python is not installed"

```bash
pip install llama-cpp-python
```

### Erreur: "Model file not found"

1. V√©rifier que le mod√®le est t√©l√©charg√© :
```bash
ls -lh models/enrichment/*.gguf
```

2. T√©l√©charger manuellement :
```bash
python -c "
from infrastructure.models.model_manager import ModelManager
ModelManager('./models/enrichment').download_model('phi-3-mini')
"
```

### Performance lente

1. V√©rifier le nombre de threads :
```python
import os
print(f"CPU cores: {os.cpu_count()}")
```

2. Augmenter `n_batch` (si RAM suffisante) :
```ini
n_batch = 1024
```

3. R√©duire `n_ctx` (si contexte court suffit) :
```ini
n_ctx = 1024
```

### M√©moire insuffisante

1. Utiliser un mod√®le plus petit (gemma-2b au lieu de phi-3-mini)
2. R√©duire `n_ctx` et `n_batch`
3. R√©duire `cache_max_models` √† 1

## üìö Ressources

- [llama.cpp Documentation](https://github.com/ggerganov/llama.cpp)
- [llama-cpp-python GitHub](https://github.com/abetlen/llama-cpp-python)
- [Hugging Face GGUF Models](https://huggingface.co/models?library=gguf)
