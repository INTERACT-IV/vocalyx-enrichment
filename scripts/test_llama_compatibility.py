#!/usr/bin/env python3
"""
Script pour tester la compatibilit√© de llama-cpp-python avec un mod√®le GGUF
"""

import sys
from pathlib import Path

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

def test_llama_version():
    """Teste la version de llama-cpp-python"""
    try:
        import llama_cpp
        version = getattr(llama_cpp, '__version__', 'unknown')
        print(f"‚úÖ llama-cpp-python version: {version}")
        
        # V√©rifier la version minimale
        try:
            from packaging import version as v
            if v.parse(version) >= v.parse("0.2.20"):
                print(f"   ‚úÖ Version compatible avec Qwen 2.5 (>= 0.2.20)")
            else:
                print(f"   ‚ö†Ô∏è  Version trop ancienne pour Qwen 2.5 (recommand√©: >= 0.2.20)")
                print(f"   üí° Mettre √† jour: pip install --upgrade llama-cpp-python")
        except ImportError:
            print(f"   ‚ö†Ô∏è  Impossible de v√©rifier la version (packaging non install√©)")
        
        return True, version
    except ImportError:
        print("‚ùå llama-cpp-python n'est pas install√©")
        print("   üí° Installer avec: pip install llama-cpp-python")
        return False, None


def test_model_load(model_path: str):
    """Teste le chargement d'un mod√®le"""
    from llama_cpp import Llama
    
    model_path_obj = Path(model_path)
    
    if not model_path_obj.exists():
        print(f"‚ùå Fichier n'existe pas: {model_path}")
        return False
    
    print(f"\nüîç Test de chargement du mod√®le: {model_path}")
    print(f"   Taille: {model_path_obj.stat().st_size / (1024**3):.2f} GB")
    
    try:
        # Essayer de charger avec des param√®tres minimaux
        print("   Tentative de chargement...")
        llm = Llama(
            model_path=str(model_path),
            n_ctx=512,  # Contexte minimal pour test
            n_threads=1,
            n_batch=128,
            n_gpu_layers=0,
            verbose=False,
            use_mmap=True,
            use_mlock=False
        )
        print("   ‚úÖ Mod√®le charg√© avec succ√®s!")
        
        # Test de g√©n√©ration simple
        print("   Test de g√©n√©ration...")
        response = llm("Bonjour", max_tokens=10, stop=["\n"], echo=False)
        if response and 'choices' in response and len(response['choices']) > 0:
            generated = response['choices'][0].get('text', '').strip()
            print(f"   ‚úÖ G√©n√©ration r√©ussie: '{generated[:50]}...'")
        else:
            print(f"   ‚ö†Ô∏è  G√©n√©ration retourn√©e mais format inattendu")
        
        del llm
        return True
        
    except ValueError as e:
        print(f"   ‚ùå Erreur ValueError: {e}")
        print(f"   üí° Possible causes:")
        print(f"      - Version de llama-cpp-python incompatible")
        print(f"      - Fichier corrompu")
        print(f"      - Format GGUF incompatible")
        return False
    except Exception as e:
        print(f"   ‚ùå Erreur: {type(e).__name__}: {e}")
        return False


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Teste la compatibilit√© de llama-cpp-python avec un mod√®le GGUF'
    )
    parser.add_argument(
        'model_path',
        type=str,
        help='Chemin vers le fichier GGUF √† tester'
    )
    
    args = parser.parse_args()
    
    print("üß™ Test de compatibilit√© llama-cpp-python\n")
    
    # Test 1: Version
    print("1Ô∏è‚É£ V√©rification de la version:")
    is_installed, version = test_llama_version()
    
    if not is_installed:
        sys.exit(1)
    
    # Test 2: Chargement du mod√®le
    print("\n2Ô∏è‚É£ Test de chargement du mod√®le:")
    success = test_model_load(args.model_path)
    
    if success:
        print("\n‚úÖ Tous les tests ont r√©ussi!")
        sys.exit(0)
    else:
        print("\n‚ùå √âchec du test de chargement")
        print("\nüí° Solutions possibles:")
        print("   1. Mettre √† jour llama-cpp-python:")
        print("      pip install --upgrade llama-cpp-python")
        print("   2. Avec optimisations CPU:")
        print("      CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install --upgrade llama-cpp-python")
        print("   3. V√©rifier que le fichier GGUF n'est pas corrompu")
        print("   4. Essayer un autre mod√®le (mistral-7b-instruct ou phi-3-mini)")
        sys.exit(1)


if __name__ == '__main__':
    main()

