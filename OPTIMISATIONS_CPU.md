# üöÄ Pistes d'am√©lioration pour acc√©l√©rer l'enrichissement sur CPU

Ce document liste les principales pistes d'optimisation pour am√©liorer les performances de l'enrichissement sur CPU.

## üìä Analyse de l'√©tat actuel

### Points de blocage identifi√©s

1. **G√©n√©ration s√©quentielle des m√©tadonn√©es** (worker.py:434-496)
   - Titre, r√©sum√©, satisfaction et bullet_points sont g√©n√©r√©s un par un
   - Temps total = somme des temps individuels
   - Impact : ~4x plus lent que n√©cessaire

2. **Correction de texte segment par segment** (enrichment_service.py:466-544)
   - Chaque segment est trait√© individuellement dans une boucle
   - Pas de traitement par batch
   - Impact : Latence √©lev√©e pour les transcriptions longues

3. **Param√®tres CPU non optimis√©s**
   - `n_threads` : Auto-d√©tection (CPU_COUNT - 1)
   - `n_batch` : 512 (peut √™tre augment√©)
   - `n_ctx` : 2048 (peut √™tre r√©duit selon les besoins)

4. **Pas de cache de r√©sultats**
   - M√™me texte = m√™me traitement r√©p√©t√©
   - Pas de mise en cache des m√©tadonn√©es g√©n√©r√©es

5. **Taille du contexte non optimis√©e**
   - Texte complet envoy√© pour toutes les m√©tadonn√©es
   - Certaines t√¢ches (titre) n'ont besoin que d'un √©chantillon

---

## üéØ Pistes d'am√©lioration (par ordre de priorit√©)

### 1. ‚ö° Parall√©lisation de la g√©n√©ration des m√©tadonn√©es

**Impact estim√© : 3-4x plus rapide pour les m√©tadonn√©es**

**Probl√®me actuel :**
```python
# worker.py:434-496 - G√©n√©ration s√©quentielle
title = enrichment_service.generate_metadata(...)  # ~2-5s
summary = enrichment_service.generate_metadata(...)  # ~3-7s
satisfaction = enrichment_service.generate_metadata(...)  # ~2-5s
bullet_points = enrichment_service.generate_metadata(...)  # ~3-7s
# Total: ~10-24s
```

**Solution :**
Utiliser `ThreadPoolExecutor` ou `concurrent.futures` pour parall√©liser les 4 appels LLM.

**Impl√©mentation :**
```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def generate_metadata_parallel(enrichment_service, text, final_prompts):
    """G√©n√®re les m√©tadonn√©es en parall√®le"""
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(enrichment_service.generate_metadata, text, "title", final_prompts, 50): "title",
            executor.submit(enrichment_service.generate_metadata, text, "summary", final_prompts, 150): "summary",
            executor.submit(enrichment_service.generate_metadata, text, "satisfaction", final_prompts, 100): "satisfaction",
            executor.submit(enrichment_service.generate_metadata, text, "bullet_points", final_prompts, 200): "bullet_points"
        }
        
        results = {}
        for future in as_completed(futures):
            task_type = futures[future]
            try:
                results[task_type] = future.result()
            except Exception as e:
                logger.warning(f"Failed to generate {task_type}: {e}")
                results[task_type] = None
        return results
```

**Gain attendu :** 3-4x plus rapide (de ~15s √† ~5s)

---

### 2. üì¶ Traitement par batch des segments

**Impact estim√© : 2-3x plus rapide pour la correction de texte**

**Probl√®me actuel :**
```python
# enrichment_service.py:487-536 - Traitement s√©quentiel
for segment in segments:
    enriched_text = self._generate_text(prompt, ...)  # Appel LLM par segment
```

**Solution :**
Grouper plusieurs segments courts en un seul batch pour r√©duire le nombre d'appels LLM.

**Impl√©mentation :**
```python
def enrich_segments_batch(self, segments, batch_size=5):
    """Enrichit les segments par batch"""
    enriched_segments = []
    
    # Grouper les segments en batches
    for i in range(0, len(segments), batch_size):
        batch = segments[i:i+batch_size]
        batch_text = "\n".join([f"Segment {j+1}: {seg.get('text', '')}" for j, seg in enumerate(batch)])
        
        # Un seul appel LLM pour le batch
        corrected_batch = self._generate_text(
            f"{base_instructions}\n\nCorrige ces segments:\n{batch_text}",
            max_tokens=len(batch_text.split()) * 2,
            temperature=0.05
        )
        
        # Parser et distribuer les r√©sultats
        corrected_segments = self._parse_batch_response(corrected_batch, batch)
        enriched_segments.extend(corrected_segments)
    
    return enriched_segments
```

**Gain attendu :** 2-3x plus rapide pour les transcriptions avec beaucoup de segments courts

---

### 3. üîß Optimisation des param√®tres CPU

**Impact estim√© : 10-30% d'am√©lioration**

**Param√®tres √† ajuster :**

#### a) `n_threads` - Nombre de threads
```python
# Actuel: max(1, os.cpu_count() - 1)
# Optimis√©: Utiliser tous les cores disponibles si RAM suffisante
self.n_threads = os.cpu_count() or 4  # Utiliser tous les cores
```

**Note :** Pour les mod√®les GGUF, utiliser tous les cores peut √™tre b√©n√©fique si la RAM est suffisante.

#### b) `n_batch` - Taille du batch
```python
# Actuel: 512
# Optimis√©: Augmenter si RAM disponible
self.n_batch = 1024  # ou 2048 si RAM > 16GB
```

**Impact :** R√©duit la latence en traitant plus de tokens √† la fois.

#### c) `n_ctx` - Taille du contexte
```python
# Actuel: 2048
# Optimis√©: R√©duire selon les besoins
self.n_ctx = 1024  # Pour les transcriptions courtes (< 500 mots)
# ou 1536 pour un compromis
```

**Impact :** R√©duit la m√©moire utilis√©e et acc√©l√®re le traitement.

**Configuration recommand√©e :**
```python
# Pour CPU avec 8+ cores et 16GB+ RAM
n_threads = os.cpu_count()
n_batch = 1024
n_ctx = 1536

# Pour CPU avec 4 cores et 8GB RAM
n_threads = 3  # Laisser 1 core libre
n_batch = 512
n_ctx = 1024
```

---

### 4. üéØ R√©duction intelligente du contexte

**Impact estim√© : 20-40% d'am√©lioration pour certaines t√¢ches**

**Probl√®me actuel :**
- Le texte complet est envoy√© pour toutes les m√©tadonn√©es
- Le titre n'a besoin que d'un √©chantillon (d√©j√† fait : `text[:500]`)
- Le r√©sum√© peut utiliser un √©chantillon intelligent

**Solution :**
```python
def get_smart_sample(text, task_type, max_chars=1000):
    """Extrait un √©chantillon intelligent du texte"""
    if task_type == "title":
        # Prendre le d√©but (contexte initial)
        return text[:500]
    elif task_type == "summary":
        # Prendre d√©but + milieu + fin (structure narrative)
        if len(text) <= max_chars:
            return text
        third = len(text) // 3
        return f"{text[:third]}...\n\n{text[third:2*third]}...\n\n{text[2*third:]}"
    elif task_type == "satisfaction":
        # Prendre le texte complet (analyse globale n√©cessaire)
        return text[:2000]  # Limiter quand m√™me
    else:
        return text[:max_chars]
```

**Gain attendu :** R√©duction de 20-40% du temps de traitement pour les transcriptions longues

---

### 5. üíæ Cache de r√©sultats

**Impact estim√© : 100% plus rapide pour les textes identiques**

**Solution :**
Mettre en cache les r√©sultats d'enrichissement bas√©s sur un hash du texte.

**Impl√©mentation :**
```python
import hashlib
import json
from functools import lru_cache

class EnrichmentCache:
    def __init__(self, redis_client=None, ttl=3600):
        self.redis = redis_client
        self.ttl = ttl
        self.local_cache = {}  # Cache local (LRU)
    
    def _get_hash(self, text, task_type):
        """G√©n√®re un hash du texte + type de t√¢che"""
        content = f"{task_type}:{text[:500]}"  # Limiter pour le hash
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, text, task_type):
        """R√©cup√®re un r√©sultat depuis le cache"""
        cache_key = f"enrichment:{self._get_hash(text, task_type)}"
        
        # V√©rifier le cache local
        if cache_key in self.local_cache:
            return self.local_cache[cache_key]
        
        # V√©rifier Redis
        if self.redis:
            cached = self.redis.get(cache_key)
            if cached:
                result = json.loads(cached)
                self.local_cache[cache_key] = result
                return result
        
        return None
    
    def set(self, text, task_type, result):
        """Stocke un r√©sultat dans le cache"""
        cache_key = f"enrichment:{self._get_hash(text, task_type)}"
        
        # Stocker localement
        self.local_cache[cache_key] = result
        
        # Stocker dans Redis
        if self.redis:
            self.redis.setex(
                cache_key,
                self.ttl,
                json.dumps(result)
            )
```

**Utilisation :**
```python
def generate_metadata(self, text, task_type, prompts, max_tokens=100):
    # V√©rifier le cache
    cached = self.cache.get(text, task_type)
    if cached:
        logger.debug(f"Cache hit for {task_type}")
        return cached
    
    # G√©n√©rer normalement
    result = self._generate_text(...)
    
    # Mettre en cache
    self.cache.set(text, task_type, result)
    return result
```

**Gain attendu :** Instantan√© pour les textes d√©j√† trait√©s

---

### 6. üîÑ Optimisation des prompts

**Impact estim√© : 10-20% d'am√©lioration**

**Probl√®me actuel :**
Les prompts sont assez longs et r√©p√©titifs.

**Solution :**
- R√©duire la taille des prompts
- Utiliser des templates plus courts
- √âviter les r√©p√©titions

**Exemple :**
```python
# Avant (58 mots)
DEFAULT_ENRICHMENT_PROMPTS = {
    "title": "Cette transcription provient d'un appel entre un client (appelant) et un agent de support client. G√©n√®re un titre court et accrocheur (maximum 10 mots) pour cette transcription d'appel client. IMPORTANT: R√©ponds UNIQUEMENT en fran√ßais.",
}

# Apr√®s (25 mots - 57% plus court)
DEFAULT_ENRICHMENT_PROMPTS = {
    "title": "Appel client-agent. G√©n√®re un titre court (max 10 mots) en fran√ßais:",
}
```

**Gain attendu :** 10-20% de r√©duction du temps de g√©n√©ration

---

### 7. üßµ Utilisation de mod√®les plus l√©gers/quantifi√©s

**Impact estim√© : 2-3x plus rapide**

**Solution :**
Utiliser des mod√®les plus quantifi√©s (Q3, Q4) au lieu de Q4_K_M.

**Mod√®les recommand√©s :**
- `phi-3-mini-4k-instruct-q3_K_M.gguf` (plus rapide que Q4)
- `phi-3-mini-4k-instruct-q4_0.gguf` (plus rapide que Q4_K_M)

**Trade-off :** L√©g√®re baisse de qualit√© pour gain de vitesse significatif.

**Gain attendu :** 2-3x plus rapide avec perte de qualit√© minime (< 5%)

---

### 8. üìä Pr√©-filtrage des segments vides

**Impact estim√© : 5-10% d'am√©lioration**

**Solution :**
Filtrer les segments vides avant le traitement.

```python
def enrich_segments(self, segments, ...):
    # Filtrer les segments vides
    valid_segments = [s for s in segments if s.get('text', '').strip()]
    empty_segments = [s for s in segments if not s.get('text', '').strip()]
    
    # Traiter uniquement les segments valides
    enriched_valid = self._process_segments(valid_segments, ...)
    
    # R√©assembler avec les segments vides
    return self._merge_segments(enriched_valid, empty_segments)
```

**Gain attendu :** √âvite les appels LLM inutiles

---

### 9. üîÄ Parall√©lisation au niveau Celery

**Impact estim√© : D√©j√† impl√©ment√© (mode distribu√©)**

**√âtat actuel :**
Le mode distribu√© existe d√©j√† et fonctionne bien. Les chunks sont trait√©s en parall√®le par diff√©rents workers.

**Am√©lioration possible :**
- R√©duire le seuil de distribution (actuellement 10 segments)
- Optimiser la taille des chunks pour un meilleur √©quilibre

---

### 10. üéõÔ∏è Optimisation de la temp√©rature

**Impact estim√© : 5-10% d'am√©lioration**

**Solution :**
R√©duire la temp√©rature pour les t√¢ches d√©terministes (correction, satisfaction).

```python
# Actuel
temperature = 0.7  # Pour toutes les t√¢ches

# Optimis√©
temperature_map = {
    "title": 0.5,  # Plus d√©terministe
    "summary": 0.6,
    "satisfaction": 0.3,  # Tr√®s d√©terministe (score)
    "bullet_points": 0.5,
    "correction": 0.05  # D√©j√† fait
}
```

**Gain attendu :** G√©n√©ration plus rapide et plus coh√©rente

---

## üìà Synth√®se des gains attendus

| Optimisation | Gain estim√© | Priorit√© | Complexit√© |
|-------------|-------------|----------|------------|
| 1. Parall√©lisation m√©tadonn√©es | 3-4x | üî¥ Haute | Moyenne |
| 2. Batch processing segments | 2-3x | üü† Moyenne | Moyenne |
| 3. Optimisation param√®tres CPU | 10-30% | üü† Moyenne | Faible |
| 4. R√©duction contexte | 20-40% | üü° Faible | Faible |
| 5. Cache de r√©sultats | 100% (cache hit) | üü° Faible | Moyenne |
| 6. Optimisation prompts | 10-20% | üü° Faible | Faible |
| 7. Mod√®les plus l√©gers | 2-3x | üü† Moyenne | Faible |
| 8. Pr√©-filtrage | 5-10% | üü¢ Tr√®s faible | Tr√®s faible |
| 9. Parall√©lisation Celery | D√©j√† fait | - | - |
| 10. Optimisation temp√©rature | 5-10% | üü¢ Tr√®s faible | Tr√®s faible |

**Gain total potentiel :** 5-10x plus rapide avec les optimisations prioritaires (1, 2, 3, 7)

---

## üöÄ Plan d'impl√©mentation recommand√©

### Phase 1 - Quick wins (1-2 jours)
1. ‚úÖ Optimisation param√®tres CPU (#3)
2. ‚úÖ R√©duction contexte (#4)
3. ‚úÖ Optimisation prompts (#6)
4. ‚úÖ Pr√©-filtrage segments (#8)
5. ‚úÖ Optimisation temp√©rature (#10)

**Gain attendu :** 30-50% d'am√©lioration

### Phase 2 - Optimisations majeures (3-5 jours)
1. ‚úÖ Parall√©lisation m√©tadonn√©es (#1)
2. ‚úÖ Batch processing segments (#2)
3. ‚úÖ Mod√®les plus l√©gers (#7)

**Gain attendu :** 5-8x plus rapide au total

### Phase 3 - Optimisations avanc√©es (2-3 jours)
1. ‚úÖ Cache de r√©sultats (#5)

**Gain attendu :** Am√©lioration suppl√©mentaire pour les cas r√©p√©titifs

---

## üìù Notes importantes

1. **Compatibilit√© CPU :** Toutes ces optimisations sont compatibles avec CPU uniquement
2. **M√©moire :** Augmenter `n_batch` n√©cessite plus de RAM
3. **Qualit√© :** R√©duire la temp√©rature et utiliser des mod√®les plus l√©gers peut l√©g√®rement affecter la qualit√©
4. **Tests :** Tester chaque optimisation individuellement pour mesurer l'impact r√©el

---

## üîç Monitoring recommand√©

Ajouter des m√©triques pour mesurer :
- Temps de g√©n√©ration par type de m√©tadonn√©e
- Taux de cache hit
- Utilisation CPU/RAM
- Temps total d'enrichissement

Cela permettra de valider les gains r√©els et d'ajuster les param√®tres.
