# Vocalyx Enrichment Worker

Worker Celery pour l'enrichissement de transcriptions avec modÃ¨les LLM, optimisÃ© pour CPU avec architecture distribuÃ©e.

## ğŸš€ FonctionnalitÃ©s

- **Architecture DistribuÃ©e** : DÃ©coupage intelligent en chunks et traitement parallÃ¨le
- **Cache de ModÃ¨les LRU** : RÃ©utilisation des modÃ¨les LLM pour Ã©conomiser 10-30s par requÃªte
- **Redis pour AgrÃ©gation** : Stockage temporaire avec compression (60-70% d'Ã©conomie mÃ©moire)
- **Backend LLM Production-Ready** : `llama-cpp-python` avec modÃ¨les GGUF quantisÃ©s (CPU-only)
- **Optimisations CPU** : Quantisation Q4_K_M, threading optimisÃ©, batch processing
- **ScalabilitÃ©** : Workers partagÃ©s via Celery, distribution automatique

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- Redis (DB 3 dÃ©diÃ©e pour l'enrichissement)
- CPU avec support AVX2 (la plupart des CPUs modernes)
- 4+ GB RAM (8+ GB recommandÃ© pour meilleures performances)

## ğŸ”§ Installation

Voir le guide complet : [INSTALLATION.md](INSTALLATION.md)

### Installation Rapide

```bash
# 1. Installer les dÃ©pendances
pip install -r requirements.txt

# 2. Installer llama-cpp-python avec optimisations CPU (Linux)
CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python

# 3. TÃ©lÃ©charger un modÃ¨le (optionnel, sera fait automatiquement)
python scripts/download_model.py phi-3-mini

# 4. Configurer
cp config.ini.example config.ini
# Ã‰diter config.ini selon vos besoins
```

## âš™ï¸ Configuration

Copier `config.ini.example` vers `config.ini` et adapter :

```ini
[LLM]
model = phi-3-mini  # ou chemin vers fichier .gguf
n_threads = 0       # 0 = auto-dÃ©tection
n_ctx = 2048        # Taille du contexte
n_batch = 512       # Batch size

[PERFORMANCE]
max_workers = 2
max_chunk_size = 500
enable_cache = true
cache_max_models = 2
```

## ğŸƒ DÃ©marrage

### Mode dÃ©veloppement
```bash
celery -A worker.celery_app worker --loglevel=info --concurrency=2 -Q enrichment
```

### Mode production (Docker)
Voir `docker-compose.yml` pour la configuration complÃ¨te.

## ğŸ“Š Architecture

### Mode Classique (petites transcriptions)
```
API â†’ enrich_transcription_task â†’ EnrichmentService â†’ RÃ©sultat
```

### Mode DistribuÃ© (grandes transcriptions)
```
API â†’ orchestrate_distributed_enrichment_task
  â†“
DÃ©coupage en chunks intelligents
  â†“
enrich_chunk_task (Ã—N workers en parallÃ¨le)
  â†“
Redis (stockage temporaire)
  â†“
aggregate_enrichment_chunks_task
  â†“
RÃ©sultat final
```

## ğŸ¯ Backend LLM : llama-cpp-python

### ModÃ¨les SupportÃ©s

Le worker utilise `llama-cpp-python` avec des modÃ¨les GGUF quantisÃ©s :

- **Phi-3 Mini** (recommandÃ©) : 2.3 GB, rapide, bonne qualitÃ©
- **Mistral 7B Instruct** : 4.1 GB, excellente qualitÃ©
- **Llama 3 8B Instruct** : 4.6 GB, excellente qualitÃ©
- **Phi-3 Medium** : 7.0 GB, meilleure qualitÃ©, plus lent
- **Gemma 2B** : 1.4 GB, trÃ¨s lÃ©ger, trÃ¨s rapide

### Optimisations CPU

1. **Quantisation GGUF Q4_K_M** : RÃ©duit la taille mÃ©moire de 4x
2. **Threading optimisÃ©** : Utilise tous les cores CPU disponibles
3. **Memory mapping** : Ã‰conomise la RAM
4. **Batch processing** : Traite plusieurs tokens en parallÃ¨le
5. **OpenBLAS** : AccÃ©lÃ©ration mathÃ©matique (optionnel)

## ğŸ“ˆ Performance

### Gains attendus
- **Distribution (4 workers)** : 4x plus rapide
- **Cache de modÃ¨les** : 10-30s Ã©conomisÃ©es par requÃªte
- **Quantisation GGUF** : 4x moins de mÃ©moire, 2-3x plus rapide
- **TOTAL** : 6-10x plus rapide

### Exemple
- Transcription de 100 segments
- Mode sÃ©quentiel : 200s
- Mode distribuÃ© (4 workers) : 50s
- **AccÃ©lÃ©ration : 4x**

### Benchmarks CPU (Phi-3 Mini Q4_K_M)

| CPU | Cores | Tokens/s | RAM |
|-----|-------|----------|-----|
| Intel i7-12700 | 12 | ~25-30 | 3.5 GB |
| AMD Ryzen 7 5800X | 8 | ~20-25 | 3.5 GB |
| Apple M1 | 8 | ~30-35 | 3.5 GB |
| Apple M2 | 8 | ~35-40 | 3.5 GB |

## ğŸ” Monitoring

Le worker expose des mÃ©triques de santÃ© via Celery :

```python
from celery import current_app
inspect = current_app.control.inspect()
stats = inspect.stats()
```

## ğŸ“š Structure

```
vocalyx-enrichment/
â”œâ”€â”€ worker.py                          # Worker principal avec tÃ¢ches Celery
â”œâ”€â”€ enrichment_service.py              # Service d'enrichissement LLM
â”œâ”€â”€ config.py                          # Configuration
â”œâ”€â”€ config.ini.example                 # Exemple de configuration
â”œâ”€â”€ INSTALLATION.md                    # Guide d'installation dÃ©taillÃ©
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_model.py              # Script de tÃ©lÃ©chargement de modÃ¨les
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ llm_model_cache.py         # Cache LRU des modÃ¨les
â”‚   â”‚   â””â”€â”€ model_manager.py           # Gestionnaire de modÃ¨les (tÃ©lÃ©chargement)
â”‚   â”œâ”€â”€ redis/
â”‚   â”‚   â””â”€â”€ redis_enrichment_manager.py # Gestionnaire Redis
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ api_client.py              # Client API
â””â”€â”€ application/
    â””â”€â”€ services/
        â””â”€â”€ chunk_splitter.py          # DÃ©coupage intelligent
```

## ğŸ› DÃ©pannage

### Le modÃ¨le ne se charge pas
- VÃ©rifier que le fichier .gguf existe dans `models/enrichment/`
- VÃ©rifier que le modÃ¨le est quantisÃ© (GGUF format)
- VÃ©rifier la mÃ©moire disponible (minimum 4 GB)

### Performance lente
- Augmenter le nombre de workers
- RÃ©duire `max_chunk_size` pour plus de parallÃ©lisation
- VÃ©rifier que le cache est activÃ©
- VÃ©rifier que `n_threads` est correctement configurÃ©

### Erreur de mÃ©moire
- Utiliser un modÃ¨le plus petit (gemma-2b)
- RÃ©duire `n_ctx` et `n_batch`
- RÃ©duire `cache_max_models` Ã  1

## ğŸ“ Notes

- Les modÃ¨les GGUF sont dÃ©jÃ  quantisÃ©s (Q4_K_M par dÃ©faut)
- Le cache de modÃ¨les limite la RAM utilisÃ©e (max_models=2 par dÃ©faut)
- La compression Redis rÃ©duit la mÃ©moire mais ajoute un lÃ©ger overhead CPU
- Pour production, utiliser `phi-3-mini` ou `mistral-7b-instruct` selon les besoins

## ğŸ”— Ressources

- [Guide d'Installation](INSTALLATION.md)
- [Documentation llama.cpp](https://github.com/ggerganov/llama.cpp)
- [ModÃ¨les GGUF sur Hugging Face](https://huggingface.co/models?library=gguf)
