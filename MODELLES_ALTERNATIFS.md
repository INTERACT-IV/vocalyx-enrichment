# üöÄ Mod√®les LLM Alternatifs - Plus Rapides et Plus Pr√©cis que Mistral 7B

Ce document liste les mod√®les GGUF disponibles qui sont **plus rapides** et/ou **plus pr√©cis** que Mistral 7B Instruct pour l'enrichissement sur CPU.

## üìä Comparaison des Mod√®les

| Mod√®le | Taille | Vitesse vs Mistral | Qualit√© vs Mistral | Meilleur pour |
|--------|--------|-------------------|-------------------|---------------|
| **Phi-3 Mini Q3** | 1.8 GB | **3-4x plus rapide** | L√©g√®rement inf√©rieur | T√¢ches rapides, CPU limit√© |
| **Phi-3 Mini Q4** | 2.3 GB | **2-3x plus rapide** | L√©g√®rement inf√©rieur | √âquilibre vitesse/qualit√© (d√©faut) |
| **Gemma 2B** | 1.4 GB | **4-5x plus rapide** | Inf√©rieur | T√¢ches tr√®s simples, tr√®s rapide |
| **Mistral 7B Q4_0** | 3.8 GB | **1.2-1.5x plus rapide** | Similaire | Alternative directe √† Mistral |
| **Mistral 7B Q4_K_M** | 4.1 GB | 1x (r√©f√©rence) | R√©f√©rence | √âquilibre actuel |
| **Qwen 2.5 7B** | 4.1 GB | Similaire | **Meilleur pour fran√ßais** | Transcription fran√ßaise |
| **Llama 3 8B** | 4.8 GB | L√©g√®rement plus lent | **Plus pr√©cis** | Qualit√© maximale |
| **Gemma 7B** | 4.6 GB | Similaire √† plus rapide | Similaire | Alternative moderne |
| **Phi-3 Medium** | 7.8 GB | Plus lent | **Plus pr√©cis** | Qualit√© maximale (lourd) |

## üéØ Recommandations par Cas d'Usage

### ‚ö° Priorit√© Vitesse (2-4x plus rapide)
1. **`phi-3-mini-q3`** - Le plus rapide avec qualit√© acceptable
2. **`phi-3-mini`** - Bon compromis vitesse/qualit√© (d√©faut actuel)
3. **`gemma-2b`** - Tr√®s rapide mais qualit√© r√©duite

### üéØ Priorit√© Qualit√© (plus pr√©cis que Mistral)
1. **`llama-3-8b-instruct`** - Meilleur √©quilibre qualit√©/vitesse
2. **`qwen2.5-7b-instruct`** - Excellent pour le fran√ßais
3. **`phi-3-medium`** - Le plus pr√©cis mais plus lent

### ‚öñÔ∏è √âquilibre Vitesse/Qualit√©
1. **`mistral-7b-instruct-q4_0`** - Version plus rapide de Mistral
2. **`gemma-7b`** - Alternative moderne √† Mistral

## üîß Comment Changer de Mod√®le

### Option 1: Variable d'environnement (recommand√©)
```bash
# Dans docker-compose.yml ou .env
LLM_MODEL=llama-3-8b-instruct
```

### Option 2: Fichier config.ini
```ini
[LLM]
model = llama-3-8b-instruct
```

### Option 3: T√©l√©charger un nouveau mod√®le
```bash
# T√©l√©charger Llama 3 8B
python scripts/download_model.py llama-3-8b-instruct

# T√©l√©charger Qwen 2.5 7B (excellent pour fran√ßais)
python scripts/download_model.py qwen2.5-7b-instruct
```

## üì• T√©l√©chargement des Mod√®les

Tous les mod√®les peuvent √™tre t√©l√©charg√©s automatiquement via le script :

```bash
cd vocalyx-enrichment
python scripts/download_model.py <nom_du_modele>
```

Exemples :
```bash
# Mod√®le rapide
python scripts/download_model.py phi-3-mini-q3

# Mod√®le pr√©cis
python scripts/download_model.py llama-3-8b-instruct

# Mod√®le optimis√© pour fran√ßais
python scripts/download_model.py qwen2.5-7b-instruct
```

## üß™ Tests de Performance

Pour tester les performances d'un mod√®le :

```bash
python test_enrichment.py --model <nom_du_modele>
```

## üí° Conseils d'Optimisation

### Pour CPU avec 4-8 cores et 8-16 GB RAM
- **Recommand√©** : `phi-3-mini` ou `llama-3-8b-instruct`
- **√âviter** : `phi-3-medium` (trop lourd)

### Pour CPU avec 8+ cores et 16+ GB RAM
- **Recommand√©** : `llama-3-8b-instruct` ou `qwen2.5-7b-instruct`
- **Alternative rapide** : `mistral-7b-instruct-q4_0`

### Pour CPU limit√© (2-4 cores, 4-8 GB RAM)
- **Recommand√©** : `phi-3-mini-q3` ou `gemma-2b`
- **√âviter** : Mod√®les 7B+ (trop lourds)

## üìà Benchmarks Approximatifs

Bas√©s sur des tests CPU typiques (8 cores, 16 GB RAM) :

| Mod√®le | Temps enrichissement* | Qualit√©** |
|--------|----------------------|-----------|
| Phi-3 Mini Q3 | ~3-5s | 7/10 |
| Phi-3 Mini Q4 | ~4-6s | 8/10 |
| Mistral 7B Q4_K_M | ~10-15s | 9/10 |
| Mistral 7B Q4_0 | ~8-12s | 9/10 |
| Llama 3 8B | ~12-18s | 9.5/10 |
| Qwen 2.5 7B | ~10-15s | 9/10 (10/10 pour fran√ßais) |

*Temps pour enrichir une transcription de ~500 mots (titre + r√©sum√© + score + bullets)
**Qualit√© subjective bas√©e sur la pr√©cision et la coh√©rence

## üîÑ Migration depuis Mistral 7B

### Vers un mod√®le plus rapide
```bash
# 1. T√©l√©charger le nouveau mod√®le
python scripts/download_model.py phi-3-mini-q3

# 2. Modifier la configuration
export LLM_MODEL=phi-3-mini-q3

# 3. Red√©marrer le worker
docker-compose restart vocalyx-enrichment-01
```

### Vers un mod√®le plus pr√©cis
```bash
# 1. T√©l√©charger le nouveau mod√®le
python scripts/download_model.py llama-3-8b-instruct

# 2. Modifier la configuration
export LLM_MODEL=llama-3-8b-instruct

# 3. Red√©marrer le worker
docker-compose restart vocalyx-enrichment-01
```

## ‚ö†Ô∏è Notes Importantes

1. **Premier chargement** : Le premier chargement d'un mod√®le peut prendre 10-30 secondes
2. **M√©moire** : Les mod√®les 7B+ n√©cessitent au moins 8 GB RAM
3. **Cache** : Les mod√®les sont mis en cache pour √©viter les rechargements
4. **Compatibilit√©** : Tous les mod√®les utilisent le format GGUF (compatible llama.cpp)

## üÜò D√©pannage

### Le mod√®le ne se charge pas
- V√©rifier que le mod√®le est t√©l√©charg√© : `python scripts/find_model.py <nom>`
- V√©rifier les logs : `docker-compose logs vocalyx-enrichment-01`

### Erreur de m√©moire
- Utiliser un mod√®le plus petit (Phi-3 Mini ou Gemma 2B)
- R√©duire `n_ctx` dans la configuration (ex: 1024 au lieu de 2048)

### Mod√®le trop lent
- Utiliser une version Q3 au lieu de Q4
- Utiliser `phi-3-mini-q3` ou `gemma-2b`

