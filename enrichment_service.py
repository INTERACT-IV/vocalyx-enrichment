"""
EnrichmentService - Service pour l'enrichissement des transcriptions avec LLM (llama-cpp-python)
"""

import logging
import json
import re
import os
from typing import Dict, Optional, List
from pathlib import Path
from llama_cpp import Llama

from infrastructure.models.model_manager import ModelManager

logger = logging.getLogger("vocalyx.enrichment")

# Prompts par d√©faut pour l'enrichissement
DEFAULT_ENRICHMENT_PROMPTS = {
    "title": "G√©n√®re un titre court et accrocheur (maximum 10 mots) pour cette transcription.",
    "summary": "G√©n√®re un r√©sum√© concis de moins de 100 mots pour cette transcription.",
    "satisfaction": "Analyse cette transcription et attribue un score de satisfaction client de 1 √† 10. Justifie bri√®vement ton score. Format JSON: {\"score\": nombre, \"justification\": \"texte\"}",
    "bullet_points": "Extrais les points cl√©s de cette transcription sous forme de puces. Format JSON: {\"points\": [\"point 1\", \"point 2\", ...]}"
}


class EnrichmentService:
    """Service pour enrichir les transcriptions avec un LLM local (GGUF)"""
    
    def __init__(self, config=None, model_name: str = None, models_dir: Path = None, device: str = "cpu"):
        """
        Initialise le service d'enrichissement avec un mod√®le LLM local.
        
        Args:
            config: Configuration du worker (optionnel)
            model_name: Nom du mod√®le LLM (ex: "tinyllama", "phi-3-mini") ou chemin complet
            models_dir: R√©pertoire contenant les mod√®les (d√©faut: depuis config ou /app/shared/models/enrichment)
            device: Device √† utiliser ("cpu" uniquement pour GGUF)
        """
        self.config = config
        self.model_name = model_name or (getattr(config, 'llm_model', 'tinyllama') if config else 'tinyllama')
        self.device = device or (getattr(config, 'llm_device', 'cpu') if config else 'cpu')
        
        # Gestionnaire de mod√®les
        if models_dir:
            self.models_dir = Path(models_dir)
        elif config:
            self.models_dir = Path(getattr(config, 'llm_models_dir', '/app/shared/models/enrichment'))
        else:
            self.models_dir = Path("/app/shared/models/enrichment")
        
        self.model_manager = ModelManager(models_dir=str(self.models_dir))
        
        # Param√®tres CPU depuis config
        n_threads_config = getattr(config, 'llm_n_threads', None) if config else None
        if n_threads_config == 0:
            n_threads_config = None
        self.n_threads = n_threads_config or max(1, (os.cpu_count() or 4) - 1)
        self.n_ctx = getattr(config, 'llm_n_ctx', 2048) if config else 2048
        self.n_batch = getattr(config, 'llm_n_batch', 512) if config else 512
        
        self.model_path = None
        self.llm = None
        
        logger.info(
            f"üéØ EnrichmentService initialized | "
            f"Model: {self.model_name} | "
            f"Device: {self.device} | "
            f"Threads: {self.n_threads} | "
            f"Context: {self.n_ctx}"
        )
    
    def _load_model(self):
        """Charge le mod√®le LLM GGUF via llama-cpp-python (lazy loading)"""
        if self.llm is not None:
            return
        
        try:
            logger.info(f"üöÄ Loading LLM model: {self.model_name}...")
            
            # Obtenir le chemin du mod√®le via le gestionnaire
            model_path = self.model_manager.get_model_path(self.model_name)
            
            if not model_path.exists():
                logger.warning(f"‚ö†Ô∏è Model not found at: {model_path}")
                # Essayer de t√©l√©charger si c'est un mod√®le recommand√©
                if self.model_name in self.model_manager.RECOMMENDED_MODELS:
                    logger.info(f"üì• Attempting to download model {self.model_name}...")
                    try:
                        model_path = self.model_manager.download_model(self.model_name)
                    except Exception as download_error:
                        logger.error(f"‚ùå Failed to download model: {download_error}")
                        raise FileNotFoundError(f"Model file not found: {model_path}")
                else:
                    raise FileNotFoundError(f"Model file not found: {model_path}")
            
            # V√©rifier la sant√© du mod√®le
            if not self.model_manager.check_model_health(model_path):
                raise ValueError(f"Model health check failed: {model_path}")
            
            self.model_path = model_path
            model_path_str = str(model_path.absolute())
            
            logger.info(
                f"üì¶ Loading GGUF model | "
                f"Path: {model_path_str} | "
                f"Threads: {self.n_threads} | "
                f"Context: {self.n_ctx} | "
                f"Batch: {self.n_batch}"
            )
            
            # Charger le mod√®le GGUF avec llama-cpp-python
            self.llm = Llama(
                model_path=model_path_str,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                n_batch=self.n_batch,
                n_gpu_layers=0,  # CPU only
                verbose=False,
                use_mmap=True,
                use_mlock=False
            )
            
            logger.info(f"‚úÖ Model {self.model_name} loaded successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to load model {self.model_name}: {e}", exc_info=True)
            raise
    
    def _generate_text(self, prompt: str, max_tokens: int = 200, temperature: float = 0.7, stop_tokens: List[str] = None) -> str:
        """
        G√©n√®re du texte avec le mod√®le LLM.
        
        Args:
            prompt: Prompt √† envoyer au mod√®le
            max_tokens: Nombre maximum de tokens √† g√©n√©rer
            temperature: Temp√©rature pour la g√©n√©ration (0.0-1.0)
            stop_tokens: Liste de tokens d'arr√™t (d√©faut: selon le mod√®le)
            
        Returns:
            str: Texte g√©n√©r√©
        """
        try:
            self._load_model()
            
            # D√©terminer les tokens d'arr√™t selon le mod√®le
            if stop_tokens is None:
                model_lower = self.model_name.lower()
                if 'tinyllama' in model_lower:
                    stop_tokens = ["</s>", "<|user|>", "<|system|>", "<|assistant|>", "\n\n\n"]
                elif 'phi-3' in model_lower or 'phi3' in model_lower:
                    stop_tokens = ["<|end|>", "<|user|>", "<|system|>", "<|assistant|>", "\n\n\n"]
                elif 'mistral' in model_lower:
                    stop_tokens = ["</s>", "[INST]", "[/INST]", "\n\n\n"]
                else:
                    stop_tokens = ["</s>", "<|im_end|>", "<|im_start|>", "\n\n\n"]
            
            # Formater le prompt selon le mod√®le
            model_lower = self.model_name.lower()
            if 'tinyllama' in model_lower:
                formatted_prompt = f"<|system|>\n{prompt}</s>\n<|assistant|>\n"
            elif 'phi-3' in model_lower or 'phi3' in model_lower:
                formatted_prompt = f"<|system|>\n{prompt}<|end|>\n<|assistant|>\n"
            elif 'mistral' in model_lower:
                formatted_prompt = f"<s>[INST] {prompt} [/INST]"
            else:
                formatted_prompt = prompt
            
            # G√©n√©rer la r√©ponse
            response = self.llm(
                formatted_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stop=stop_tokens,
                echo=False
            )
            
            # Extraire le texte g√©n√©r√©
            if isinstance(response, dict):
                generated_text = response.get('choices', [{}])[0].get('text', '').strip()
            elif hasattr(response, 'choices') and len(response.choices) > 0:
                generated_text = response.choices[0].text.strip()
            else:
                generated_text = str(response).strip()
            
            # Nettoyer les tokens sp√©ciaux
            tokens_to_remove = [
                '</s>', '<|end|>', '<|user|>', '<|system|>', '<|assistant|>',
                '<|im_start|>', '<|im_end|>', '[INST]', '[/INST]', '<s>', '</s>'
            ]
            for token in tokens_to_remove:
                generated_text = generated_text.replace(token, '')
            
            # Nettoyer les espaces
            generated_text = re.sub(r'\n{2,}', '\n', generated_text)
            generated_text = re.sub(r' {2,}', ' ', generated_text)
            generated_text = generated_text.strip()
            
            return generated_text
        except Exception as e:
            logger.error(f"Error generating text: {e}", exc_info=True)
            raise
    
    def generate_title(self, transcription_text: str, custom_prompt: Optional[str] = None) -> str:
        """
        G√©n√®re un titre pour la transcription.
        
        Args:
            transcription_text: Texte de la transcription
            custom_prompt: Prompt personnalis√© (optionnel)
            
        Returns:
            str: Titre g√©n√©r√©
        """
        prompt = custom_prompt or "G√©n√®re un titre court et accrocheur (maximum 10 mots) pour cette transcription:"
        full_prompt = f"{prompt}\n\n{transcription_text[:500]}"
        
        try:
            title = self._generate_text(full_prompt, max_tokens=30, temperature=0.7)
            # Nettoyer le titre (prendre la premi√®re phrase, max 10 mots)
            words = title.split()[:10]
            return " ".join(words)
        except Exception as e:
            logger.error(f"Error generating title: {e}")
            return "Titre g√©n√©r√© automatiquement"
    
    def generate_summary(self, transcription_text: str, custom_prompt: Optional[str] = None) -> str:
        """
        G√©n√®re un r√©sum√© de moins de 100 mots.
        
        Args:
            transcription_text: Texte de la transcription
            custom_prompt: Prompt personnalis√© (optionnel)
            
        Returns:
            str: R√©sum√© g√©n√©r√©
        """
        prompt = custom_prompt or "G√©n√®re un r√©sum√© concis de moins de 100 mots pour cette transcription:"
        full_prompt = f"{prompt}\n\n{transcription_text}"
        
        try:
            summary = self._generate_text(full_prompt, max_tokens=150, temperature=0.7)
            # Limiter √† 100 mots
            words = summary.split()[:100]
            return " ".join(words)
        except Exception as e:
            logger.error(f"Error generating summary: {e}")
            return "R√©sum√© g√©n√©r√© automatiquement"
    
    def generate_satisfaction_score(self, transcription_text: str, custom_prompt: Optional[str] = None) -> Dict:
        """
        G√©n√®re un score de satisfaction de 1 √† 10 avec justification.
        
        Args:
            transcription_text: Texte de la transcription
            custom_prompt: Prompt personnalis√© (optionnel)
            
        Returns:
            dict: {"score": int, "justification": str}
        """
        prompt = custom_prompt or "Analyse cette transcription et attribue un score de satisfaction client de 1 √† 10. R√©ponds en JSON: {\"score\": nombre}"
        full_prompt = f"{prompt}\n\n{transcription_text}"
        
        try:
            response = self._generate_text(full_prompt, max_tokens=100, temperature=0.5)
            
            # Essayer d'extraire le JSON de la r√©ponse
            try:
                start = response.find('{')
                end = response.rfind('}') + 1
                if start >= 0 and end > start:
                    json_str = response[start:end]
                    data = json.loads(json_str)
                    return {
                        "score": int(data.get("score", 5))
                    }
            except:
                pass
            
            # Fallback: extraire un score simple
            score_match = re.search(r'\b([1-9]|10)\b', response)
            score = int(score_match.group(1)) if score_match else 5
            return {
                "score": score
            }
        except Exception as e:
            logger.error(f"Error generating satisfaction score: {e}")
            return {"score": 5}
    
    def generate_bullet_points(self, transcription_text: str, custom_prompt: Optional[str] = None) -> list:
        """
        G√©n√®re des bullet points pour la transcription.
        
        Args:
            transcription_text: Texte de la transcription
            custom_prompt: Prompt personnalis√© (optionnel)
            
        Returns:
            list: Liste de bullet points
        """
        prompt = custom_prompt or "Extrais les points cl√©s de cette transcription sous forme de puces. R√©ponds en JSON: {\"points\": [\"point 1\", \"point 2\", ...]}"
        full_prompt = f"{prompt}\n\n{transcription_text}"
        
        try:
            response = self._generate_text(full_prompt, max_tokens=200, temperature=0.7)
            
            # Essayer d'extraire le JSON de la r√©ponse
            try:
                start = response.find('{')
                end = response.rfind('}') + 1
                if start >= 0 and end > start:
                    json_str = response[start:end]
                    data = json.loads(json_str)
                    return data.get("points", [])
            except:
                pass
            
            # Fallback: extraire les points avec regex
            points = re.findall(r'[-‚Ä¢*]\s*(.+?)(?=\n|$)', response)
            if not points:
                # Essayer d'extraire des lignes num√©rot√©es
                points = re.findall(r'\d+[\.\)]\s*(.+?)(?=\n|$)', response)
            return points[:4] if points else ["Point cl√© g√©n√©r√© automatiquement"]
        except Exception as e:
            logger.error(f"Error generating bullet points: {e}")
            return ["Point cl√© g√©n√©r√© automatiquement"]
    
    def enrich_transcription(
        self,
        transcription_text: str,
        prompts: Optional[Dict[str, str]] = None
    ) -> Dict:
        """
        Enrichit une transcription compl√®te en g√©n√©rant titre, r√©sum√©, score et bullet points.
        
        Args:
            transcription_text: Texte de la transcription
            prompts: Dictionnaire avec les prompts personnalis√©s (optionnel)
            
        Returns:
            dict: Donn√©es d'enrichissement avec temps individuels
        """
        import time
        
        logger.info("Starting enrichment...")
        enrichment_start_time = time.time()
        
        # Utiliser les prompts personnalis√©s ou les defaults
        title_prompt = prompts.get("title") if prompts and isinstance(prompts, dict) else None
        summary_prompt = prompts.get("summary") if prompts and isinstance(prompts, dict) else None
        satisfaction_prompt = prompts.get("satisfaction") if prompts and isinstance(prompts, dict) else None
        bullet_points_prompt = prompts.get("bullet_points") if prompts and isinstance(prompts, dict) else None
        
        # G√©n√©rer tous les √©l√©ments avec mesure du temps
        logger.info("Generating title...")
        title_start = time.time()
        title = self.generate_title(transcription_text, title_prompt)
        title_time = round(time.time() - title_start, 2)
        
        logger.info("Generating summary...")
        summary_start = time.time()
        summary = self.generate_summary(transcription_text, summary_prompt)
        summary_time = round(time.time() - summary_start, 2)
        
        logger.info("Generating satisfaction score...")
        satisfaction_start = time.time()
        satisfaction = self.generate_satisfaction_score(transcription_text, satisfaction_prompt)
        satisfaction_time = round(time.time() - satisfaction_start, 2)
        
        logger.info("Generating bullet points...")
        bullet_points_start = time.time()
        bullet_points = self.generate_bullet_points(transcription_text, bullet_points_prompt)
        bullet_points_time = round(time.time() - bullet_points_start, 2)
        
        total_enrichment_time = round(time.time() - enrichment_start_time, 2)
        
        enrichment_data = {
            "title": title,
            "summary": summary,
            "satisfaction_score": satisfaction["score"],
            "bullet_points": bullet_points[:4],  # Limiter √† 4 points maximum
            "timing": {
                "title_time": title_time,
                "summary_time": summary_time,
                "satisfaction_time": satisfaction_time,
                "bullet_points_time": bullet_points_time,
                "total_time": total_enrichment_time
            }
        }
        
        logger.info(f"‚úÖ Enrichment completed in {total_enrichment_time}s (title: {title_time}s, summary: {summary_time}s, score: {satisfaction_time}s, bullets: {bullet_points_time}s)")
        return enrichment_data
    
    def enrich_segments(self, segments: List[Dict], context: Optional[List[Dict]] = None, custom_prompts: Optional[Dict] = None) -> List[Dict]:
        """
        Enrichit une liste de segments de transcription (correction du texte).
        
        Args:
            segments: Liste de segments √† enrichir
            context: Segments pr√©c√©dents pour le contexte (non utilis√© pour la correction)
            custom_prompts: Prompts personnalis√©s (non utilis√© pour la correction)
            
        Returns:
            Liste de segments enrichis avec 'enriched_text'
        """
        if not segments:
            return []
        
        try:
            self._load_model()
            
            enriched_segments = []
            previous_text = None
            
            for i, segment in enumerate(segments):
                text = segment.get('text', '').strip()
                if not text:
                    enriched_segments.append(segment)
                    continue
                
                # Construire le prompt pour la correction
                base_instructions = (
                    "Tu es un assistant qui CORRIGE et AM√âLIORE des transcriptions audio en fran√ßais. "
                    "R√àGLES STRICTES :\n"
                    "1. Corriger UNIQUEMENT les erreurs d'orthographe et de grammaire\n"
                    "2. Am√©liorer UNIQUEMENT la ponctuation (points, virgules, majuscules)\n"
                    "3. Am√©liorer UNIQUEMENT la structure (majuscules en d√©but de phrase)\n"
                    "4. CONSERVER EXACTEMENT le sens original - ne rien ajouter, ne rien inventer\n"
                    "5. Retourner UNIQUEMENT le texte corrig√©, sans explications\n"
                    "6. La longueur du texte corrig√© doit √™tre SIMILAIRE √† l'original"
                )
                task_instruction = "Corrige et am√©liore ce texte de transcription en conservant le sens original. Retourne UNIQUEMENT le texte corrig√©:"
                prompt = f"{base_instructions}\n\n{task_instruction}\n\nTexte:\n{text}"
                
                # G√©n√©rer avec temp√©rature tr√®s basse pour √™tre d√©terministe
                estimated_tokens = len(text.split())
                max_tokens_for_text = min(256, max(50, int(estimated_tokens * 1.2)))
                
                enriched_text = self._generate_text(
                    prompt,
                    max_tokens=max_tokens_for_text,
                    temperature=0.05,  # Tr√®s bas pour correction
                    stop_tokens=["</s>", "<|end|>", "<|user|>", "<|system|>", "<|assistant|>", "\n\n\n"]
                )
                
                # V√©rifier la longueur (d√©tection d'hallucination)
                if not enriched_text:
                    enriched_text = text
                else:
                    length_ratio = len(enriched_text) / len(text) if len(text) > 0 else 1.0
                    if length_ratio > 1.5 or length_ratio < 0.5:
                        logger.warning(
                            f"‚ö†Ô∏è Model generated suspicious output (length mismatch: "
                            f"input={len(text)} chars, output={len(enriched_text)} chars), "
                            f"returning original text"
                        )
                        enriched_text = text
                
                # Cr√©er le segment enrichi
                enriched_segment = segment.copy()
                enriched_segment['enriched_text'] = enriched_text
                enriched_segment['original_text'] = text
                
                enriched_segments.append(enriched_segment)
                previous_text = text
            
            logger.info(f"‚úÖ Enriched {len(enriched_segments)} segments")
            return enriched_segments
            
        except Exception as e:
            logger.error(f"‚ùå Error enriching segments: {e}", exc_info=True)
            return segments
    
    def generate_metadata(self, text: str, task_type: str, prompts: Dict[str, str], max_tokens: int = 100) -> str:
        """
        G√©n√®re des m√©tadonn√©es (titre, r√©sum√©, score, bullet points) √† partir du texte.
        
        Args:
            text: Texte de la transcription
            task_type: Type de m√©tadonn√©e ("title", "summary", "satisfaction", "bullet_points")
            prompts: Dict avec les prompts (par d√©faut + personnalis√©s)
            max_tokens: Nombre maximum de tokens √† g√©n√©rer
            
        Returns:
            Texte g√©n√©r√© (m√©tadonn√©e)
        """
        if not text or not text.strip():
            return ""
        
        try:
            self._load_model()
            
            # Obtenir le prompt pour cette t√¢che
            prompt_text = prompts.get(task_type, DEFAULT_ENRICHMENT_PROMPTS.get(task_type, ""))
            
            # Construire le prompt complet
            if task_type == "title":
                full_prompt = f"{prompt_text}\n\n{text[:500]}"
                result = self.generate_title(text, prompt_text)
                return result
            elif task_type == "summary":
                full_prompt = f"{prompt_text}\n\n{text}"
                result = self.generate_summary(text, prompt_text)
                return result
            elif task_type == "satisfaction":
                full_prompt = f"{prompt_text}\n\n{text}"
                result = self.generate_satisfaction_score(text, prompt_text)
                # Retourner en format JSON string
                return json.dumps(result, ensure_ascii=False)
            elif task_type == "bullet_points":
                full_prompt = f"{prompt_text}\n\n{text}"
                result = self.generate_bullet_points(text, prompt_text)
                # Retourner en format JSON string
                return json.dumps({"points": result}, ensure_ascii=False)
            else:
                # Fallback: utiliser _generate_text directement
                full_prompt = f"{prompt_text}\n\n{text}"
                return self._generate_text(full_prompt, max_tokens=max_tokens, temperature=0.7)
            
        except Exception as e:
            logger.error(f"‚ùå Error generating metadata '{task_type}': {e}", exc_info=True)
            return ""
    
    def cleanup(self):
        """Nettoie les ressources du mod√®le"""
        if self.llm is not None:
            try:
                del self.llm
                self.llm = None
                import gc
                gc.collect()
                logger.info("üßπ EnrichmentService cleaned up")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error during cleanup: {e}")
                self.llm = None