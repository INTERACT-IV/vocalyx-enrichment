"""
Service d'enrichissement de transcription avec mod√®les LLM
Optimis√© pour CPU avec quantisation GGUF via llama-cpp-python
Production-ready pour environnement CPU-only
"""

import logging
import os
from pathlib import Path
from typing import List, Dict, Optional

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

from infrastructure.models.model_manager import ModelManager

logger = logging.getLogger("vocalyx")

if not PSUTIL_AVAILABLE:
    logger.warning("‚ö†Ô∏è psutil not available, memory monitoring disabled")


class EnrichmentService:
    """Service d'enrichissement utilisant des mod√®les LLM via llama-cpp-python"""
    
    def __init__(self, config, model_name: str = None):
        """
        Initialise le service d'enrichissement.
        
        Args:
            config: Configuration du worker
            model_name: Nom du mod√®le LLM √† utiliser (chemin vers fichier .gguf)
        """
        self.config = config
        self.model_name = model_name or getattr(config, 'llm_model', 'phi-3-mini')
        self.model = None
        self.tokenizer = None
        self.device = getattr(config, 'llm_device', 'cpu')
        self.compute_type = getattr(config, 'llm_compute_type', 'int8')
        self.max_tokens = getattr(config, 'llm_max_tokens', 256)
        self.temperature = getattr(config, 'llm_temperature', 0.7)
        self.top_p = getattr(config, 'llm_top_p', 0.9)
        self.top_k = getattr(config, 'llm_top_k', 40)
        
        # Param√®tres CPU
        n_threads_config = getattr(config, 'llm_n_threads', None)
        if n_threads_config == 0:
            n_threads_config = None
        self.n_threads = n_threads_config  # Auto-d√©tect√© si None
        self.n_ctx = getattr(config, 'llm_n_ctx', 2048)  # Contexte maximum
        self.n_batch = getattr(config, 'llm_n_batch', 512)  # Batch size pour CPU
        
        # D√©tecter le nombre de threads CPU si non sp√©cifi√©
        if self.n_threads is None:
            cpu_count = os.cpu_count() or 4
            # Utiliser tous les cores sauf 1 pour laisser de la marge
            self.n_threads = max(1, cpu_count - 1)
        
        # Gestionnaire de mod√®les
        # Par d√©faut, utiliser /app/shared/models/enrichment (Docker) comme transcription
        models_dir = getattr(config, 'llm_models_dir', '/app/shared/models/enrichment')
        self.model_manager = ModelManager(models_dir=models_dir)
        
        logger.info(
            f"üéØ EnrichmentService initialized | "
            f"Model: {self.model_name} | "
            f"Device: {self.device} | "
            f"Compute: {self.compute_type} | "
            f"Threads: {self.n_threads} | "
            f"Context: {self.n_ctx}"
        )
    
    def _load_model(self):
        """Charge le mod√®le LLM GGUF via llama-cpp-python (lazy loading)"""
        if self.model is not None:
            return
        
        try:
            logger.info(f"üöÄ Loading LLM model: {self.model_name}...")
            
            # Obtenir le chemin du mod√®le via le gestionnaire
            model_path = self.model_manager.get_model_path(self.model_name)
            
            # V√©rifier si le mod√®le existe
            if not model_path.exists():
                logger.warning(
                    f"‚ö†Ô∏è Model not found at: {model_path}\n"
                    f"   Searching in shared directories..."
                )
                
                # Le gestionnaire a d√©j√† cherch√© dans shared/, mais on peut essayer de t√©l√©charger
                # seulement si c'est un mod√®le recommand√© et qu'on a vraiment besoin
                if self.model_name in self.model_manager.RECOMMENDED_MODELS:
                    logger.info(f"üì• Model not found locally, attempting download...")
                    try:
                        model_path = self.model_manager.download_model(self.model_name)
                    except Exception as download_error:
                        logger.error(
                            f"‚ùå Failed to download model {self.model_name}: {download_error}\n"
                            f"   Please ensure the model file exists in:\n"
                            f"   - {self.model_manager.models_dir}\n"
                            f"   - /app/shared/models/enrichment/ (Docker)\n"
                            f"   - ./shared/models/enrichment/ (local)\n"
                            f"   Or provide the full path to the .gguf file."
                        )
                        raise
                else:
                    raise FileNotFoundError(
                        f"Model file not found: {model_path}\n"
                        f"Please ensure the model exists or provide the correct path."
                    )
            
            # V√©rifier la sant√© du mod√®le
            if not self.model_manager.check_model_health(model_path):
                raise ValueError(f"Model health check failed: {model_path}")
            
            model_path_str = str(model_path.absolute())
            
            # Importer llama-cpp-python
            try:
                from llama_cpp import Llama
            except ImportError as e:
                import sys
                raise ImportError(
                    f"llama-cpp-python is not installed or not accessible.\n"
                    f"Error: {e}\n"
                    f"Python: {sys.executable}\n"
                    f"Python version: {sys.version}\n"
                    f"Install it with: pip3 install llama-cpp-python\n"
                    f"Or verify installation with: python3 -c 'import llama_cpp'"
                )
            
            # D√©terminer le nombre de threads GPU (0 pour CPU-only)
            n_gpu_layers = 0  # CPU-only
            
            # Charger le mod√®le avec optimisations CPU
            logger.info(
                f"üì¶ Loading GGUF model | "
                f"Path: {model_path_str} | "
                f"Threads: {self.n_threads} | "
                f"Context: {self.n_ctx} | "
                f"Batch: {self.n_batch}"
            )
            
            self.model = Llama(
                model_path=model_path_str,
                n_ctx=self.n_ctx,  # Taille du contexte
                n_threads=self.n_threads,  # Nombre de threads CPU
                n_batch=self.n_batch,  # Taille du batch
                n_gpu_layers=n_gpu_layers,  # 0 = CPU-only
                verbose=False,  # D√©sactiver les logs verbeux de llama.cpp
                use_mmap=True,  # Memory mapping pour √©conomiser la RAM
                use_mlock=False,  # Ne pas verrouiller en m√©moire (permet swap si n√©cessaire)
            )
            
            # V√©rifier la m√©moire utilis√©e (si psutil disponible)
            if PSUTIL_AVAILABLE:
                mem_info = psutil.virtual_memory()
                logger.info(
                    f"‚úÖ LLM model loaded successfully | "
                    f"Memory used: {mem_info.used / 1024**3:.2f} GB / {mem_info.total / 1024**3:.2f} GB "
                    f"({mem_info.percent:.1f}%)"
                )
            else:
                logger.info("‚úÖ LLM model loaded successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load LLM model {self.model_name}: {e}", exc_info=True)
            raise
    
    def enrich_text(self, text: str, context: Optional[str] = None) -> str:
        """
        Enrichit un texte avec le mod√®le LLM.
        
        Args:
            text: Texte √† enrichir
            context: Contexte optionnel (texte pr√©c√©dent)
            
        Returns:
            Texte enrichi
        """
        if not text or not text.strip():
            return text
        
        try:
            self._load_model()
            
            # Construire le prompt
            prompt = self._build_prompt(text, context)
            
            # G√©n√©rer avec le mod√®le
            # Utiliser les param√®tres optimis√©s pour CPU
            response = self.model(
                prompt,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                top_p=self.top_p,
                top_k=self.top_k,
                repeat_penalty=1.1,  # √âviter la r√©p√©tition
                stop=["</s>", "\n\n\n"],  # Tokens d'arr√™t
                echo=False,  # Ne pas retourner le prompt
            )
            
            # Extraire le texte g√©n√©r√©
            if isinstance(response, dict):
                enriched_text = response.get('choices', [{}])[0].get('text', '').strip()
            else:
                # Fallback si le format est diff√©rent
                enriched_text = str(response).strip()
            
            # Nettoyer le texte (supprimer les tokens d'arr√™t qui auraient pu passer)
            enriched_text = enriched_text.replace('</s>', '').strip()
            
            # Si le mod√®le n'a rien g√©n√©r√© ou a g√©n√©r√© quelque chose de suspect,
            # retourner le texte original
            if not enriched_text or len(enriched_text) < len(text) * 0.5:
                logger.warning(
                    f"‚ö†Ô∏è Model generated suspicious output (too short or empty), "
                    f"returning original text"
                )
                return text
            
            logger.debug(
                f"‚úÖ Text enriched | "
                f"Input: {len(text)} chars | "
                f"Output: {len(enriched_text)} chars"
            )
            return enriched_text
            
        except Exception as e:
            logger.error(f"‚ùå Error enriching text: {e}", exc_info=True)
            # En cas d'erreur, retourner le texte original
            return text
    
    def enrich_segments(self, segments: List[Dict], context: Optional[List[Dict]] = None) -> List[Dict]:
        """
        Enrichit une liste de segments de transcription.
        
        Args:
            segments: Liste de segments √† enrichir
            context: Segments pr√©c√©dents pour le contexte
            
        Returns:
            Liste de segments enrichis
        """
        if not segments:
            return []
        
        try:
            self._load_model()
            
            enriched_segments = []
            previous_text = None
            
            for i, segment in enumerate(segments):
                text = segment.get('text', '').strip()
                if not text:
                    enriched_segments.append(segment)
                    continue
                
                # Utiliser le texte pr√©c√©dent comme contexte
                context_text = previous_text if context is None else None
                
                # Enrichir le segment
                enriched_text = self.enrich_text(text, context_text)
                
                # Cr√©er le segment enrichi
                enriched_segment = segment.copy()
                enriched_segment['enriched_text'] = enriched_text
                enriched_segment['original_text'] = text
                
                enriched_segments.append(enriched_segment)
                previous_text = text
            
            logger.info(f"‚úÖ Enriched {len(enriched_segments)} segments")
            return enriched_segments
            
        except Exception as e:
            logger.error(f"‚ùå Error enriching segments: {e}", exc_info=True)
            # En cas d'erreur, retourner les segments originaux
            return segments
    
    def _build_prompt(self, text: str, context: Optional[str] = None) -> str:
        """
        Construit le prompt pour le mod√®le LLM.
        Adapt√© pour les mod√®les instruct (Phi-3, Mistral, Llama, etc.)
        
        Args:
            text: Texte √† enrichir
            context: Contexte optionnel
            
        Returns:
            Prompt format√© selon le format du mod√®le
        """
        # D√©tecter le type de mod√®le depuis le nom
        model_lower = self.model_name.lower()
        
        # Format pour Phi-3
        if 'phi-3' in model_lower or 'phi3' in model_lower:
            system_prompt = "Tu es un assistant qui am√©liore et enrichit des transcriptions audio en fran√ßais. Tu corriges les erreurs, am√©liores la ponctuation et la structure, tout en conservant le sens original."
            if context:
                user_prompt = f"Contexte pr√©c√©dent: {context}\n\nTexte √† enrichir: {text}\n\nEnrichis ce texte:"
            else:
                user_prompt = f"Enrichis et am√©liore ce texte de transcription: {text}"
            
            prompt = f"<|system|>\n{system_prompt}<|end|>\n<|user|>\n{user_prompt}<|end|>\n<|assistant|>\n"
        
        # Format pour Mistral
        elif 'mistral' in model_lower:
            system_prompt = "Tu es un assistant qui am√©liore et enrichit des transcriptions audio en fran√ßais."
            if context:
                user_prompt = f"Contexte: {context}\n\nTexte: {text}\n\nEnrichis ce texte:"
            else:
                user_prompt = f"Enrichis et am√©liore ce texte de transcription: {text}"
            
            prompt = f"<s>[INST] {system_prompt}\n\n{user_prompt} [/INST]"
        
        # Format pour Llama 3
        elif 'llama-3' in model_lower or 'llama3' in model_lower:
            system_prompt = "Tu es un assistant qui am√©liore et enrichit des transcriptions audio en fran√ßais."
            if context:
                user_prompt = f"Contexte: {context}\n\nTexte: {text}\n\nEnrichis ce texte:"
            else:
                user_prompt = f"Enrichis et am√©liore ce texte de transcription: {text}"
            
            prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        
        # Format g√©n√©rique (ChatML)
        else:
            system_prompt = "Tu es un assistant qui am√©liore et enrichit des transcriptions audio en fran√ßais. Tu corriges les erreurs, am√©liores la ponctuation et la structure, tout en conservant le sens original."
            if context:
                user_prompt = f"Contexte pr√©c√©dent: {context}\n\nTexte √† enrichir: {text}\n\nEnrichis ce texte:"
            else:
                user_prompt = f"Enrichis et am√©liore ce texte de transcription: {text}"
            
            prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        return prompt
    
    def cleanup(self):
        """Nettoie les ressources du mod√®le"""
        if self.model is not None:
            try:
                # llama-cpp-python lib√®re automatiquement les ressources
                # mais on peut forcer la lib√©ration
                del self.model
                self.model = None
                
                # Forcer le garbage collection
                import gc
                gc.collect()
                
                logger.info("üßπ EnrichmentService cleaned up")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error during cleanup: {e}")
                self.model = None
